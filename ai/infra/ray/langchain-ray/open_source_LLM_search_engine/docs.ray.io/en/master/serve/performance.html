
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Performance Tuning &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css@digest=1999514e3f237ded88cf.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css@digest=5115cc725059bd94278eecd172e13a965bf8f5a9.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script src="../_static/js/rate-the-docs.es.min.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js@digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="../../../_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/serve/performance.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Handling Dependencies" href="handling-dependencies.html" />
    <link rel="prev" title="Adding End-to-End Fault Tolerance" href="production-guide/fault-tolerance.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>


  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="../../../_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": false, "api_host": "https://readthedocs.com", "build_date": "2023-04-28T22:33:26Z", "builder": "sphinx", "canonical_url": null, "commit": "ff36b8e7", "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-2", "language": "en", "page": "serve/performance", "programming_language": "py", "project": "anyscale-ray", "proxied_api_host": "/_", "source_suffix": ".md", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="../../../_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-air/getting-started.html">
   Ray AI Runtime (AIR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray Serve
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="getting_started.html">
     Getting Started
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="user-guide.html">
     User Guides
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="http-guide.html">
       HTTP Handling
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="scaling-and-resource-allocation.html">
       Scaling and Resource Allocation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="model_composition.html">
       Model Composition
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="dev-workflow.html">
       Development Workflow
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="multi-app.html">
       Deploying Multiple Serve Applications
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="production-guide/index.html">
       Production Guide
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="performance.html#">
       Performance Tuning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="handling-dependencies.html">
       Handling Dependencies
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="managing-java-deployments.html">
       Experimental Java API
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="migration.html">
       1.x to 2.x API Migration Guide
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="direct-ingress.html">
       Experimental Direct Ingress
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="architecture.html">
     Architecture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tutorials/index.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="api/index.html">
     Ray Serve API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/monitoring-debugging/monitoring-debugging.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Fserve/performance.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/serve/performance.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/serve/performance.md.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="performance.html#performance-and-known-benchmarks">
   Performance and known benchmarks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="performance.html#request-batching">
   Request Batching
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#enable-batching-for-your-deployment">
     Enable batching for your deployment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#tips-for-fine-tuning-batching-parameters">
     Tips for fine-tuning batching parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="performance.html#debugging-performance-issues">
   Debugging performance issues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#using-async-methods">
     Using
     <code class="docutils literal notranslate">
      <span class="pre">
       async
      </span>
     </code>
     methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#set-an-end-to-end-request-timeout">
     Set an end-to-end request timeout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#set-request-retry-times">
     Set request retry times
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Performance Tuning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="performance.html#performance-and-known-benchmarks">
   Performance and known benchmarks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="performance.html#request-batching">
   Request Batching
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#enable-batching-for-your-deployment">
     Enable batching for your deployment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#tips-for-fine-tuning-batching-parameters">
     Tips for fine-tuning batching parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="performance.html#debugging-performance-issues">
   Debugging performance issues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#using-async-methods">
     Using
     <code class="docutils literal notranslate">
      <span class="pre">
       async
      </span>
     </code>
     methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#set-an-end-to-end-request-timeout">
     Set an end-to-end request timeout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="performance.html#set-request-retry-times">
     Set request retry times
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="performance-tuning">
<h1><a class="toc-backref" href="performance.html#id1">Performance Tuning</a><a class="headerlink" href="performance.html#performance-tuning" title="Permalink to this headline">#</a></h1>
<p>This section should help you:</p>
<ul class="simple">
<li><p>understand Ray Serve’s performance characteristics</p></li>
<li><p>find ways to debug and tune your Serve application’s performance</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section offers some tips and tricks to improve your Ray Serve application’s performance. Check out the <a class="reference internal" href="architecture.html#serve-architecture"><span class="std std-ref">architecture page</span></a> for helpful context, including an overview of the HTTP proxy actor and deployment replica actors.</p>
</div>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="performance.html#performance-tuning" id="id1">Performance Tuning</a></p>
<ul>
<li><p><a class="reference internal" href="performance.html#performance-and-known-benchmarks" id="id2">Performance and known benchmarks</a></p></li>
<li><p><a class="reference internal" href="performance.html#request-batching" id="id3">Request Batching</a></p>
<ul>
<li><p><a class="reference internal" href="performance.html#enable-batching-for-your-deployment" id="id4">Enable batching for your deployment</a></p></li>
<li><p><a class="reference internal" href="performance.html#tips-for-fine-tuning-batching-parameters" id="id5">Tips for fine-tuning batching parameters</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="performance.html#debugging-performance-issues" id="id6">Debugging performance issues</a></p>
<ul>
<li><p><a class="reference internal" href="performance.html#using-async-methods" id="id7">Using <code class="docutils literal notranslate"><span class="pre">async</span></code> methods</a></p></li>
<li><p><a class="reference internal" href="performance.html#set-an-end-to-end-request-timeout" id="id8">Set an end-to-end request timeout</a></p></li>
<li><p><a class="reference internal" href="performance.html#set-request-retry-times" id="id9">Set request retry times</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<section id="performance-and-known-benchmarks">
<h2><a class="toc-backref" href="performance.html#id2">Performance and known benchmarks</a><a class="headerlink" href="performance.html#performance-and-known-benchmarks" title="Permalink to this headline">#</a></h2>
<p>We are continuously benchmarking Ray Serve. The metrics we care about are latency, throughput, and scalability. We can confidently say:</p>
<ul class="simple">
<li><p>Ray Serve’s latency overhead is single digit milliseconds, around 1-2 milliseconds on average.</p></li>
<li><p>For throughput, Serve achieves about 3-4k queries per second on a single machine (8 cores) using 1 HTTP proxy actor and 8 replicas performing no-op requests.</p></li>
<li><p>It is horizontally scalable so you can add more machines to increase the overall throughput. Ray Serve is built on top of Ray,
so its scalability is bounded by Ray’s scalability. Please see Ray’s <a class="reference external" href="https://github.com/ray-project/ray/blob/master/release/benchmarks/README.md">scalability envelope</a>
to learn more about the maximum number of nodes and other limitations.</p></li>
</ul>
<p>We run long-running benchmarks nightly:</p>
<table class="table">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Benchmark</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Cluster Details</p></th>
<th class="head"><p>Performance Numbers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ray-project/ray/blob/de227ac407d6cad52a4ead09571eff6b1da73a6d/release/serve_tests/workloads/single_deployment_1k_noop_replica.py">Single Deployment</a></p></td>
<td><p>Runs 10 minute <a class="reference external" href="https://github.com/wg/wrk">wrk</a> trial on a single no-op deployment with 1000 replicas.</p></td>
<td><p>Head node: AWS EC2 m5.8xlarge. 32 worker nodes: AWS EC2 m5.8xlarge.</p></td>
<td><ul class="simple">
<li><p>per_thread_latency_avg_ms = 22.41</p></li>
<li><p>per_thread_latency_max_ms = 1400.0</p></li>
<li><p>per_thread_avg_tps = 55.75</p></li>
<li><p>per_thread_max_tps = 121.0</p></li>
<li><p>per_node_avg_tps = 553.17</p></li>
<li><p>per_node_avg_transfer_per_sec_KB = 83.19</p></li>
<li><p>cluster_total_thoughput = 10954456</p></li>
<li><p>cluster_total_transfer_KB = 1647441.9199999997</p></li>
<li><p>cluster_total_timeout_requests = 0</p></li>
<li><p>cluster_max_P50_latency_ms = 8.84</p></li>
<li><p>cluster_max_P75_latency_ms = 35.31</p></li>
<li><p>cluster_max_P90_latency_ms = 49.69</p></li>
<li><p>cluster_max_P99_latency_ms = 56.5</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/ray-project/ray/blob/de227ac407d6cad52a4ead09571eff6b1da73a6d/release/serve_tests/workloads/multi_deployment_1k_noop_replica.py">Multiple Deployments</a></p></td>
<td><p>Runs 10 minute <a class="reference external" href="https://github.com/wg/wrk">wrk</a> trial on 10 deployments with 100 replicas each. Each deployment recursively sends queries to up to 5 other deployments.</p></td>
<td><p>Head node: AWS EC2 m5.8xlarge. 32 worker nodes: AWS EC2 m5.8xlarge.</p></td>
<td><ul class="simple">
<li><p>per_thread_latency_avg_ms = 0.0</p></li>
<li><p>per_thread_latency_max_ms = 0.0</p></li>
<li><p>per_thread_avg_tps = 0.0</p></li>
<li><p>per_thread_max_tps = 0.0</p></li>
<li><p>per_node_avg_tps = 0.35</p></li>
<li><p>per_node_avg_transfer_per_sec_KB = 0.05</p></li>
<li><p>cluster_total_thoughput = 6964</p></li>
<li><p>cluster_total_transfer_KB = 1047.28</p></li>
<li><p>cluster_total_timeout_requests = 6964.0</p></li>
<li><p>cluster_max_P50_latency_ms = 0.0</p></li>
<li><p>cluster_max_P75_latency_ms = 0.0</p></li>
<li><p>cluster_max_P90_latency_ms = 0.0</p></li>
<li><p>cluster_max_P99_latency_ms = 0.0</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/ray-project/ray/blob/f6735f90c72581baf83a9cea7cbbe3ea2f6a56d8/release/serve_tests/workloads/deployment_graph_wide_ensemble.py">Deployment Graph: Ensemble</a></p></td>
<td><p>Runs 10 node ensemble, constructed with a call graph, that performs basic arithmetic at each node. Ensemble pattern routes the input to 10 different nodes, and their outputs are combined to produce the final output. Simulates 4 clients making 20 requests each.</p></td>
<td><p>Head node: AWS EC2 m5.8xlarge. 0 Worker nodes.</p></td>
<td><ul class="simple">
<li><p>throughput_mean_tps = 8.75</p></li>
<li><p>throughput_std_tps = 0.43</p></li>
<li><p>latency_mean_ms = 126.15</p></li>
<li><p>latency_std_ms = 18.35</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The performance numbers above come from a recent run of the nightly benchmarks.</p>
</div>
<!--- See https://github.com/ray-project/ray/pull/27711 for more context on the benchmarks. -->
<p>Check out <a class="reference external" href="https://github.com/ray-project/ray/tree/f6735f90c72581baf83a9cea7cbbe3ea2f6a56d8/release/serve_tests/workloads">our benchmark workloads’</a> source code directly to get a better sense of what they test. You can see which cluster templates each benchmark uses <a class="reference external" href="https://github.com/ray-project/ray/blob/8eca6ae852e2d23bcf49680fef6f0384a1b63564/release/release_tests.yaml#L2328-L2576">here</a> (under the <code class="docutils literal notranslate"><span class="pre">cluster_compute</span></code> key), and you can see what type of nodes each template spins up <a class="reference external" href="https://github.com/ray-project/ray/tree/8beb887bbed31ecea3d2813b61833b81c45712e1/release/serve_tests">here</a>.</p>
<p>You can check out our <a class="reference external" href="https://github.com/ray-project/ray/blob/master/python/ray/serve/benchmarks/README.md">microbenchmark instructions</a>
to benchmark Ray Serve on your hardware.</p>
</section>
<section id="request-batching">
<span id="serve-performance-batching-requests"></span><h2><a class="toc-backref" href="performance.html#id3">Request Batching</a><a class="headerlink" href="performance.html#request-batching" title="Permalink to this headline">#</a></h2>
<p>Serve offers a request batching feature that can improve your service throughput without sacrificing latency. This is possible because ML models can utilize efficient vectorized computation to process a batch of request at a time. Batching is also necessary when your model is expensive to use and you want to maximize the utilization of hardware.</p>
<p>Machine Learning (ML) frameworks such as Tensorflow, PyTorch, and Scikit-Learn support evaluating multiple samples at the same time.
Ray Serve allows you to take advantage of this feature via dynamic request batching.
When a request arrives, Serve puts the request in a queue. This queue buffers the requests to form a batch. The deployment picks up the batch and evaluates it. After the evaluation, the resulting batch will be split up, and each response is returned individually.</p>
<section id="enable-batching-for-your-deployment">
<h3><a class="toc-backref" href="performance.html#id4">Enable batching for your deployment</a><a class="headerlink" href="performance.html#enable-batching-for-your-deployment" title="Permalink to this headline">#</a></h3>
<p>You can enable batching by using the <a class="reference internal" href="api/doc/ray.serve.batch.html#ray.serve.batch" title="ray.serve.batch"><code class="xref py py-mod docutils literal notranslate"><span class="pre">ray.serve.batch</span></code></a> decorator. Let’s take a look at a simple example by modifying the <code class="docutils literal notranslate"><span class="pre">MyModel</span></code> class to accept a batch.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">serve</span>
<span class="kn">import</span> <span class="nn">ray</span>


<span class="nd">@serve</span><span class="o">.</span><span class="n">deployment</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">single_sample</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">single_sample</span> <span class="o">*</span> <span class="mi">2</span>


<span class="n">handle</span> <span class="o">=</span> <span class="n">serve</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="n">bind</span><span class="p">())</span>
<span class="k">assert</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
<p>The batching decorators expect you to make the following changes in your method signature:</p>
<ul class="simple">
<li><p>The method is declared as an async method because the decorator batches in asyncio event loop.</p></li>
<li><p>The method accepts a list of its original input types as input. For example, <code class="docutils literal notranslate"><span class="pre">arg1:</span> <span class="pre">int,</span> <span class="pre">arg2:</span> <span class="pre">str</span></code> should be changed to <code class="docutils literal notranslate"><span class="pre">arg1:</span> <span class="pre">List[int],</span> <span class="pre">arg2:</span> <span class="pre">List[str]</span></code>.</p></li>
<li><p>The method returns a list. The length of the return list and the input list must be of equal lengths for the decorator to split the output evenly and return a corresponding response back to its respective request.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">serve</span>
<span class="kn">import</span> <span class="nn">ray</span>

<span class="hll">
</span><span class="hll"><span class="nd">@serve</span><span class="o">.</span><span class="n">deployment</span>
</span><span class="hll"><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
</span><span class="hll">    <span class="nd">@serve</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">max_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">batch_wait_timeout_s</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span>    <span class="k">async</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">multiple_samples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># Use numpy&#39;s vectorized computation to efficiently process a batch.</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">multiple_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>


<span class="n">handle</span> <span class="o">=</span> <span class="n">serve</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">Model</span><span class="o">.</span><span class="n">bind</span><span class="p">())</span>
<span class="k">assert</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">([</span><span class="n">handle</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span> <span class="o">==</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)]</span>
</pre></div>
</div>
<p>You can supply two optional parameters to the decorators.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batch_wait_timeout_s</span></code> controls how long Serve should wait for a batch once the first request arrives.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code> controls the size of the batch.
Once the first request arrives, the batching decorator will wait for a full batch (up to <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>) until <code class="docutils literal notranslate"><span class="pre">batch_wait_timeout_s</span></code> is reached. If the timeout is reached, the batch will be sent to the model regardless the batch size.</p></li>
</ul>
</section>
<section id="tips-for-fine-tuning-batching-parameters">
<h3><a class="toc-backref" href="performance.html#id5">Tips for fine-tuning batching parameters</a><a class="headerlink" href="performance.html#tips-for-fine-tuning-batching-parameters" title="Permalink to this headline">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code> ideally should be a power of 2 (2, 4, 8, 16, …) because CPUs and GPUs are both optimized for data of these shapes. Large batch sizes incur a high memory cost as well as latency penalty for the first few requests.</p>
<p><code class="docutils literal notranslate"><span class="pre">batch_wait_timeout_s</span></code> should be set considering the end to end latency SLO (Service Level Objective). For example, if your latency target is 150ms, and the model takes 100ms to evaluate the batch, the <code class="docutils literal notranslate"><span class="pre">batch_wait_timeout_s</span></code> should be set to a value much lower than 150ms - 100ms = 50ms.</p>
<p>When using batching in a Serve Deployment Graph, the relationship between an upstream node and a downstream node might affect the performance as well. Consider a chain of two models where first model sets <code class="docutils literal notranslate"><span class="pre">max_batch_size=8</span></code> and second model sets <code class="docutils literal notranslate"><span class="pre">max_batch_size=6</span></code>. In this scenario, when the first model finishes a full batch of 8, the second model will finish one batch of 6 and then to fill the next batch, which will initially only be partially filled with 8 - 6 = 2 requests, incurring latency costs. The batch size of downstream models should ideally be multiples or divisors of the upstream models to ensure the batches play well together.</p>
</section>
</section>
<section id="debugging-performance-issues">
<h2><a class="toc-backref" href="performance.html#id6">Debugging performance issues</a><a class="headerlink" href="performance.html#debugging-performance-issues" title="Permalink to this headline">#</a></h2>
<p>The performance issue you’re most likely to encounter is high latency and/or low throughput for requests.</p>
<p>Once you set up <a class="reference internal" href="production-guide/monitoring.html#serve-monitoring"><span class="std std-ref">monitoring</span></a> with Ray and Ray Serve, these issues may appear as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">serve_num_router_requests</span></code> staying constant while your load increases</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">serve_deployment_processing_latency_ms</span></code> spiking up as queries queue up in the background</p></li>
</ul>
<p>There are handful of ways to address these issues:</p>
<ol class="simple">
<li><p>Make sure you are using the right hardware and resources:</p>
<ul class="simple">
<li><p>Are you reserving GPUs for your deployment replicas using <code class="docutils literal notranslate"><span class="pre">ray_actor_options</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">ray_actor_options={“num_gpus”:</span> <span class="pre">1}</span></code>)?</p></li>
<li><p>Are you reserving one or more cores for your deployment replicas using <code class="docutils literal notranslate"><span class="pre">ray_actor_options</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">ray_actor_options={“num_cpus”:</span> <span class="pre">2}</span></code>)?</p></li>
<li><p>Are you setting <a class="reference internal" href="scaling-and-resource-allocation.html#serve-omp-num-threads"><span class="std std-ref">OMP_NUM_THREADS</span></a> to increase the performance of your deep learning framework?</p></li>
</ul>
</li>
<li><p>Try batching your requests. See <a class="reference internal" href="performance.html#serve-performance-batching-requests"><span class="std std-ref">the section above</span></a>.</p></li>
<li><p>Consider using <code class="docutils literal notranslate"><span class="pre">async</span></code> methods in your callable. See <a class="reference internal" href="performance.html#serve-performance-async-methods"><span class="std std-ref">the section below</span></a>.</p></li>
<li><p>Set an end-to-end timeout for your HTTP requests. See <a class="reference internal" href="performance.html#serve-performance-e2e-timeout"><span class="std std-ref">the section below</span></a>.</p></li>
</ol>
<section id="using-async-methods">
<span id="serve-performance-async-methods"></span><h3><a class="toc-backref" href="performance.html#id7">Using <code class="docutils literal notranslate"><span class="pre">async</span></code> methods</a><a class="headerlink" href="performance.html#using-async-methods" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>According to the <a class="reference external" href="https://fastapi.tiangolo.com/async/#very-technical-details">FastAPI documentation</a>, <code class="docutils literal notranslate"><span class="pre">def</span></code> endpoint functions will be called in a separate threadpool, so you might observe many requests running at the same time inside one replica, and this scenario might cause OOM or resource starvation. In this case, you can try to use <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code> to control the workload performance.</p>
</div>
<p>Are you using <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code> in your callable? If you are using <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> and
hitting the same queuing issue mentioned above, you might want to increase
<code class="docutils literal notranslate"><span class="pre">max_concurrent_queries</span></code>. Serve sets a low number (100) by default so the client gets
proper backpressure. You can increase the value in the deployment decorator; e.g.
<code class="docutils literal notranslate"><span class="pre">&#64;serve.deployment(max_concurrent_queries=1000)</span></code>.</p>
</section>
<section id="set-an-end-to-end-request-timeout">
<span id="serve-performance-e2e-timeout"></span><h3><a class="toc-backref" href="performance.html#id8">Set an end-to-end request timeout</a><a class="headerlink" href="performance.html#set-an-end-to-end-request-timeout" title="Permalink to this headline">#</a></h3>
<p>By default, Serve lets client HTTP requests run to completion no matter how long they take. However, slow requests could bottleneck the replica processing, blocking other requests that are waiting. It’s recommended that you set an end-to-end timeout, so slow requests can be terminated and retried at another replica.</p>
<p>You can set an end-to-end timeout for HTTP requests by setting the <code class="docutils literal notranslate"><span class="pre">RAY_SERVE_REQUEST_PROCESSING_TIMEOUT_S</span></code> environment variable. HTTP Proxies will wait for that many seconds before terminating an HTTP request and retrying it at another replica. This environment variable should be set on every node in your Ray cluster, and it cannot be updated during runtime.</p>
</section>
<section id="set-request-retry-times">
<span id="serve-performance-http-retry"></span><h3><a class="toc-backref" href="performance.html#id9">Set request retry times</a><a class="headerlink" href="performance.html#set-request-retry-times" title="Permalink to this headline">#</a></h3>
<p>By default, the Serve HTTP proxy retries up to <code class="docutils literal notranslate"><span class="pre">10</span></code> times when a response is not received due to failures (e.g. network disconnect, request timeout, etc.).</p>
<p>You can tune the number of retries by setting the <code class="docutils literal notranslate"><span class="pre">RAY_SERVE_HTTP_REQUEST_MAX_RETRIES</span></code> environment variable. The HTTP proxy will retry that many times at most before sending a task error response to the client. The number of retries will affect the end-to-end latency of your HTTP requests because Ray Serve does exponential backoff for retries.</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="production-guide/fault-tolerance.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Adding End-to-End Fault Tolerance</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="handling-dependencies.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Handling Dependencies</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js@digest=1999514e3f237ded88cf"></script>


  </body>
</html>